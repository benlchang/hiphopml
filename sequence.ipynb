{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (4.10.0.84)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mediapipe in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from jax->mediapipe) (1.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\benlc\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benlc\\hiphopml\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy opencv-python\n",
    "%pip install --user mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import mediapipe as media\n",
    "\n",
    "mp_drawing_styles = media.solutions.drawing_styles\n",
    "mp_drawing = media.solutions.drawing_utils\n",
    "mp_pose = media.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MOTION HISTORY IMAGES #######################################\n",
    "#                                                               #\n",
    "#                                                               #\n",
    "#                                                               #\n",
    "#                                                               #\n",
    "#                                                               #\n",
    "#                                                               #\n",
    "#                                                               # \n",
    "#                                                               #\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values to define tracking characteristics\n",
    "\n",
    "MHI_DURATION = 2\n",
    "THRESHOLD = 32\n",
    "MAX_TIME_DELTA = 1.0\n",
    "MIN_TIME_DELTA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_MHI_frame(frame):\n",
    "    H, W = frame.shape[0], frame.shape[1]\n",
    "    resizeH = 600\n",
    "    resizeW = int((W / H) * resizeH)\n",
    "    \n",
    "    resize_image = cv2.resize(frame, (resizeW, resizeH))\n",
    "    #input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)    #makes you blue!\n",
    "    #gray_image = cv2.cvtColor(resize_image, cv2.COLOR_BGR2GRAY)\n",
    "    return resize_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benlc\\hiphopml\\env\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "########    PROCESS LIVE WEBCAM DATA    ########\n",
    "#   MHI of pose data\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "H, W = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "W = int(W / H * 600)\n",
    "\n",
    "motion_history = np.zeros((600, W), np.float32)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.25, min_tracking_confidence=0.25) as pose:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    prev_frame = 0\n",
    "    if ret:\n",
    "        prev_initframe = preprocess_MHI_frame(frame)\n",
    "        prev_results = pose.process(prev_initframe)\n",
    "\n",
    "        prev_frame = np.zeros(prev_initframe.shape)\n",
    "        \n",
    "        mp_drawing.draw_landmarks(\n",
    "            prev_frame,\n",
    "            prev_results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"error reading video feed\")\n",
    "            break\n",
    "\n",
    "        curr_initframe = preprocess_MHI_frame(frame)\n",
    "        pose_results = pose.process(curr_initframe)\n",
    "\n",
    "        curr_frame = np.zeros(curr_initframe.shape)\n",
    "        \n",
    "        mp_drawing.draw_landmarks(\n",
    "            curr_frame,\n",
    "            pose_results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "\n",
    "        silhouette = cv2.absdiff(curr_frame, prev_frame).astype(np.uint8)\n",
    "        silhouette = cv2.cvtColor(silhouette, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        ret, motion_mask = cv2.threshold(silhouette, THRESHOLD, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "        timestamp = cv2.getTickCount() / cv2.getTickFrequency()\n",
    "        motion_history[motion_mask == 1] = timestamp\n",
    "\n",
    "        #cv2.optflow.motempl.updateMotionHistory(motion_mask, motion_history, timestamp, MHI_DURATION)\n",
    "        mhi = np.uint8(np.clip(1 - (timestamp - motion_history) / MHI_DURATION, 0, 1) * 255)\n",
    "\n",
    "        cv2.imshow('Motion History Feed', mhi)\n",
    "\n",
    "        prev_frame = curr_frame\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### POSE ESTIMATION #############################################\n",
    "#                                                               #\n",
    "#                                                               #\n",
    "#   the following cells track 33 body keypoints to              #\n",
    "#   characterize a person's pose in each frame, then            #\n",
    "#   extracts key relationships between these points             #\n",
    "#   to distinguish each dance move from one another             #\n",
    "#                                                               # \n",
    "#                                                               #\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m########    PROCESS SAVED VIDEO DATA    ########\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#open video file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m vidURL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/training/reject.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(vidURL)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_pose\u001b[38;5;241m.\u001b[39mPose(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pose:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m#read and preprocess frame\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "########    PROCESS SAVED VIDEO DATA    ########\n",
    "\n",
    "#open video file\n",
    "vidURL = './data/training/reject.mp4'\n",
    "cap = cv2.VideoCapture(vidURL)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.25, min_tracking_confidence=0.25) as pose:\n",
    "    while cap.isOpened():\n",
    "        #read and preprocess frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "    \n",
    "        #show feed!\n",
    "        frame = preprocess_MHI_frame(frame) \n",
    "\n",
    "        results = pose.process(frame)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec = mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "        cv2.imshow('Video feed', empty_frame)\n",
    "\n",
    "#clean up and close program\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 800)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m empty_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(frame\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(frame\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#draw pose annotation\u001b[39;00m\n\u001b[0;32m     22\u001b[0m mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(\n\u001b[0;32m     23\u001b[0m     empty_frame,\n\u001b[0;32m     24\u001b[0m     results\u001b[38;5;241m.\u001b[39mpose_landmarks,\n\u001b[0;32m     25\u001b[0m     mp_pose\u001b[38;5;241m.\u001b[39mPOSE_CONNECTIONS,\n\u001b[0;32m     26\u001b[0m     landmark_drawing_spec \u001b[38;5;241m=\u001b[39m mp_drawing_styles\u001b[38;5;241m.\u001b[39mget_default_pose_landmarks_style())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\mediapipe\\python\\solution_base.py:328\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    322\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    323\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionBase can only process non-audio and non-proto-list data. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    324\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_stream_type_info[stream_name]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    325\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype is not supported yet.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (input_stream_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE_FRAME \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    327\u001b[0m       input_stream_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE):\n\u001b[1;32m--> 328\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m RGB_CHANNELS:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel rgb data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    330\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    331\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    332\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    333\u001b[0m                                data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "########    PROCESS LIVE WEBCAM DATA    ########\n",
    "\n",
    "#open capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "pose_landmarks = []\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.25, min_tracking_confidence=0.25) as pose:\n",
    "    while cap.isOpened():\n",
    "        #read and preprocess frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "            \n",
    "        frame = preprocess_MHI_frame(frame)\n",
    "        empty_frame = np.zeros(frame.shape)\n",
    "\n",
    "        print(frame.shape)\n",
    "\n",
    "        results = pose.process(frame)\n",
    "\n",
    "        #draw pose annotation\n",
    "        mp_drawing.draw_landmarks(\n",
    "            empty_frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec = mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "        \n",
    "        #show feed!\n",
    "        cv2.imshow('Live feed', empty_frame)\n",
    "    \n",
    "        #listen for 'Q' key\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            pose_landmarks = results.pose_landmarks\n",
    "            break\n",
    "\n",
    "#clean up and close program\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nose {'x': 0.621558427810669, 'y': 0.08921728283166885, 'z': -0.5160354375839233, 'vis': 0.9999528527259827}\n",
      "left_eye_inner {'x': 0.6334375739097595, 'y': 0.07345728576183319, 'z': -0.484919011592865, 'vis': 0.9999215602874756}\n",
      "left_eye {'x': 0.6378164291381836, 'y': 0.07472570985555649, 'z': -0.4850243031978607, 'vis': 0.9999128580093384}\n",
      "left_eye_outer {'x': 0.6419649720191956, 'y': 0.07605085521936417, 'z': -0.4851114749908447, 'vis': 0.999923586845398}\n",
      "right_eye_inner {'x': 0.6195680499076843, 'y': 0.0685989037156105, 'z': -0.4895278811454773, 'vis': 0.9999054074287415}\n",
      "right_eye {'x': 0.6142794489860535, 'y': 0.06626370549201965, 'z': -0.4896296560764313, 'vis': 0.9998775124549866}\n",
      "right_eye_outer {'x': 0.6092092990875244, 'y': 0.06397171318531036, 'z': -0.48960861563682556, 'vis': 0.9998661875724792}\n",
      "left_ear {'x': 0.645785391330719, 'y': 0.0829315185546875, 'z': -0.2932945787906647, 'vis': 0.9998888373374939}\n",
      "right_ear {'x': 0.6009838581085205, 'y': 0.06389220803976059, 'z': -0.3177655041217804, 'vis': 0.9997892379760742}\n",
      "mouth_left {'x': 0.6256917715072632, 'y': 0.10819751769304276, 'z': -0.4447471499443054, 'vis': 0.9999687075614929}\n",
      "mouth_right {'x': 0.6098979115486145, 'y': 0.10041282325983047, 'z': -0.45215922594070435, 'vis': 0.999962329864502}\n",
      "left_shoulder {'x': 0.6493323445320129, 'y': 0.21024011075496674, 'z': -0.1606559306383133, 'vis': 0.9999806880950928}\n",
      "right_shoulder {'x': 0.5380613207817078, 'y': 0.12587593495845795, 'z': -0.1946912705898285, 'vis': 0.9999354481697083}\n",
      "left_elbow {'x': 0.7023911476135254, 'y': 0.338627427816391, 'z': -0.1837303191423416, 'vis': 0.9964820146560669}\n",
      "right_elbow {'x': 0.4533859193325043, 'y': 0.1792890876531601, 'z': -0.2460217922925949, 'vis': 0.9890568852424622}\n",
      "left_wrist {'x': 0.6528295278549194, 'y': 0.39082100987434387, 'z': -0.4268823266029358, 'vis': 0.9877458810806274}\n",
      "right_wrist {'x': 0.3940719664096832, 'y': 0.1438639760017395, 'z': -0.524735689163208, 'vis': 0.9874544739723206}\n",
      "left_pinky {'x': 0.6350216865539551, 'y': 0.41391676664352417, 'z': -0.4887162446975708, 'vis': 0.9635221362113953}\n",
      "right_pinky {'x': 0.37885624170303345, 'y': 0.11449187248945236, 'z': -0.5909318327903748, 'vis': 0.9794825911521912}\n",
      "left_index {'x': 0.6273606419563293, 'y': 0.40009644627571106, 'z': -0.507577657699585, 'vis': 0.964292049407959}\n",
      "right_index {'x': 0.38327381014823914, 'y': 0.11200836300849915, 'z': -0.6283041834831238, 'vis': 0.9817368388175964}\n",
      "left_thumb {'x': 0.6303392648696899, 'y': 0.3892706334590912, 'z': -0.4383939504623413, 'vis': 0.9560326337814331}\n",
      "right_thumb {'x': 0.388675719499588, 'y': 0.1252284049987793, 'z': -0.5484094023704529, 'vis': 0.9770404100418091}\n",
      "left_hip {'x': 0.5678485035896301, 'y': 0.4223654568195343, 'z': 0.052931562066078186, 'vis': 0.9995967745780945}\n",
      "right_hip {'x': 0.49814099073410034, 'y': 0.4078102111816406, 'z': -0.05306166782975197, 'vis': 0.9995025396347046}\n",
      "left_knee {'x': 0.6068205237388611, 'y': 0.5715246200561523, 'z': 0.013647487387061119, 'vis': 0.971367359161377}\n",
      "right_knee {'x': 0.45874837040901184, 'y': 0.5878102779388428, 'z': -0.06299544125795364, 'vis': 0.9835042357444763}\n",
      "left_ankle {'x': 0.6151605844497681, 'y': 0.7593894600868225, 'z': 0.2689381539821625, 'vis': 0.97542804479599}\n",
      "right_ankle {'x': 0.42332690954208374, 'y': 0.7434566020965576, 'z': 0.21410317718982697, 'vis': 0.9811601638793945}\n",
      "left_heel {'x': 0.598787784576416, 'y': 0.791541576385498, 'z': 0.2869493067264557, 'vis': 0.9293818473815918}\n",
      "right_heel {'x': 0.4173694849014282, 'y': 0.7572775483131409, 'z': 0.23211684823036194, 'vis': 0.781170666217804}\n",
      "left_foot_index {'x': 0.6734011769294739, 'y': 0.807495653629303, 'z': 0.13560205698013306, 'vis': 0.968251645565033}\n",
      "right_foot_index {'x': 0.438899964094162, 'y': 0.8084189295768738, 'z': 0.043795324862003326, 'vis': 0.9573058485984802}\n"
     ]
    }
   ],
   "source": [
    "########    match pose landmarks to coordinates     ########\n",
    "\n",
    "pose_indices = ['nose', \n",
    "                'left_eye_inner', 'left_eye', 'left_eye_outer', \n",
    "                'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "                'left_ear', 'right_ear',\n",
    "                'mouth_left', 'mouth_right',\n",
    "                'left_shoulder', 'right_shoulder',\n",
    "                'left_elbow', 'right_elbow',\n",
    "                'left_wrist', 'right_wrist',\n",
    "                'left_pinky', 'right_pinky',\n",
    "                'left_index', 'right_index',\n",
    "                'left_thumb', 'right_thumb',\n",
    "                'left_hip', 'right_hip',\n",
    "                'left_knee', 'right_knee',\n",
    "                'left_ankle', 'right_ankle',\n",
    "                'left_heel', 'right_heel',\n",
    "                'left_foot_index', 'right_foot_index']\n",
    "\n",
    "def setNewCoords(landmark):\n",
    "    new_coords = [{\n",
    "                'x': point.x,\n",
    "                'y': point.y,\n",
    "                'z': point.z,\n",
    "                'vis': point.visibility\n",
    "                }   \n",
    "                for point in landmark]\n",
    "\n",
    "    keypoints = {}\n",
    "    for i in range(len(pose_indices)):\n",
    "        keypoints[pose_indices[i]] = new_coords[i]\n",
    "\n",
    "    return keypoints\n",
    "\n",
    "keypoints = setNewCoords(pose_landmarks.landmark)\n",
    "for key in keypoints.keys():\n",
    "    if keypoints[key]['vis'] > 0.5:\n",
    "        print(key, keypoints[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body-ang 0.2968405428715714\n",
      "LA-ang 1.9901597463662961\n",
      "RA-ang 2.040441019913823\n",
      "LL-ang -2.9303926250519243\n",
      "RL-ang 3.1332781667736183\n",
      "SR-ang 0.02653235396175402\n",
      "SR-mp 0.020781755447387695\n",
      "F-depth 0.0\n",
      "F-height 0.015932857990264893\n",
      "LF-ang -2.3055504388264145\n",
      "RF-ang -2.682542757841427\n"
     ]
    }
   ],
   "source": [
    "########    calculate parameter values     ########\n",
    "\n",
    "def calcParamValues(keypoints):\n",
    "    #hip midpoint\n",
    "    hip_midpoint = (keypoints['right_hip']['x'] - keypoints['left_hip']['x']) / 2\n",
    "\n",
    "    #calculate cartesian distance between two points\n",
    "    def calculateDistance(x, y):\n",
    "        return np.sqrt(x ** 2 + y ** 2)\n",
    "\n",
    "    #calculate angle created by two cartesian vectors\n",
    "    #   negative angles : bend outward, i.e. gotta piss\n",
    "    #   positive angles : bend inward, i.e. sumo squat\n",
    "    def calculateAngle(A, B, C = None):\n",
    "        direction = 1\n",
    "        if not C:\n",
    "            C = {'x': 1, 'y': 0}\n",
    "        elif abs(B['x'] - hip_midpoint) < abs(C['x'] - hip_midpoint):\n",
    "            direction = -1\n",
    "\n",
    "        vecBA, vecBC = [A['x'] - B['x'], A['y'] - B['y']], [C['x'] - B['x'], C['y'] - B['y']]\n",
    "        dot_product = vecBA[0] * vecBC[0] + vecBA[1] * vecBC[1]\n",
    "        magBA, magBC = calculateDistance(vecBA[0], vecBA[1]), calculateDistance(vecBC[0], vecBC[1])\n",
    "        return np.arccos(dot_product / (magBA * magBC)) * direction\n",
    "\n",
    "    #calculate body facing\n",
    "    # -pi : facing to the right\n",
    "    #  0  : facing the camera\n",
    "    #  pi : facing to the left\n",
    "    shoulder_depthdiff = keypoints['left_shoulder']['z'] - keypoints['right_shoulder']['z']\n",
    "    shoulder_widthdiff = keypoints['left_shoulder']['x'] - keypoints['right_shoulder']['x']\n",
    "    body_angle = np.arctan(shoulder_depthdiff / shoulder_widthdiff)\n",
    "\n",
    "    #calculate arm angles\n",
    "    leftarm_angle = calculateAngle(keypoints['left_shoulder'], keypoints['left_elbow'], keypoints['left_wrist']) \n",
    "    rightarm_angle = calculateAngle(keypoints['right_shoulder'], keypoints['right_elbow'], keypoints['right_wrist']) \n",
    "    leftleg_angle = calculateAngle(keypoints['left_hip'], keypoints['left_knee'], keypoints['left_ankle'])\n",
    "    rightleg_angle = calculateAngle(keypoints['right_hip'], keypoints['right_knee'], keypoints['right_ankle'])\n",
    "\n",
    "    #calculate shoulder dimensions relative to hips\n",
    "    shoulder_angle = calculateAngle(keypoints['left_shoulder'], keypoints['right_shoulder'])\n",
    "    hip_angle = calculateAngle(keypoints['left_hip'], keypoints['right_hip'])\n",
    "    shoulder_relativeangle = shoulder_angle - hip_angle\n",
    "\n",
    "    shoulder_midpoint = (keypoints['right_shoulder']['x'] - keypoints['left_shoulder']['x']) / 2\n",
    "    shoulder_relativemidpoint = abs(shoulder_midpoint - hip_midpoint)\n",
    "\n",
    "    #calculate foot dimensions relative to each other\n",
    "    foot_depthdiff = keypoints['left_ankle']['z'] - keypoints['left_ankle']['z']\n",
    "    foot_heightdiff = keypoints['left_ankle']['y'] - keypoints['right_ankle']['y']\n",
    "\n",
    "    #calculate angle of the foot\n",
    "    # using knee-heel-toe angle to describe foot direction\n",
    "    leftfoot_angle = calculateAngle(keypoints['left_knee'], keypoints['left_heel'], keypoints['left_foot_index'])\n",
    "    rightfoot_angle = calculateAngle(keypoints['right_knee'], keypoints['right_heel'], keypoints['right_foot_index'])\n",
    "\n",
    "    profile = {'body-ang': body_angle, \n",
    "               'LA-ang': leftarm_angle, 'RA-ang': rightarm_angle, \n",
    "               'LL-ang': leftleg_angle, 'RL-ang': rightleg_angle,\n",
    "            'SR-ang': shoulder_relativeangle, 'SR-mp': shoulder_relativemidpoint,\n",
    "            'F-depth': foot_depthdiff, 'F-height': foot_heightdiff,\n",
    "            'LF-ang': leftfoot_angle, 'RF-ang': rightfoot_angle}\n",
    "    \n",
    "    return profile\n",
    "\n",
    "parameterValues = calcParamValues(keypoints)\n",
    "for key in parameterValues.keys():\n",
    "    print(key, parameterValues[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
